{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_PXHr3eE4SF",
        "outputId": "551b2ce2-0932-48fc-cfba-a851d782fc53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: pinecone 6.0.2 does not provide the extra 'async'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet pymupdf langchain langchain-core langchain-google-genai langchain-pinecone langchain-community sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kyPfLt0rEsnI"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import re\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "class EnhancedPyMuPDFLoader(PyMuPDFLoader):\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\"Load documents and detect images\"\"\"\n",
        "        docs = super().load()\n",
        "        enhanced_docs = []\n",
        "\n",
        "        pdf_document = fitz.open(self.file_path)\n",
        "        for i, page in enumerate(pdf_document):\n",
        "            page_docs = [doc for doc in docs if doc.metadata.get('page') == i]\n",
        "\n",
        "            image_list = page.get_images(full=True)\n",
        "\n",
        "            for doc in page_docs:\n",
        "                image_placeholders = {}\n",
        "\n",
        "                for img_idx, img_info in enumerate(image_list):\n",
        "                    xref = img_info[0]\n",
        "\n",
        "                    base_image = pdf_document.extract_image(xref)\n",
        "\n",
        "                    image_id = f\"img_{i+1}_{img_idx+1}\"\n",
        "                    placeholder = f\"[IMAGE: {image_id}]\"\n",
        "                    image_metadata = {\n",
        "                        \"image_id\": image_id,\n",
        "                        \"page_num\": i + 1,\n",
        "                        \"position\": img_idx + 1,\n",
        "                        \"source_file\": self.file_path,\n",
        "                        \"image_format\": base_image[\"ext\"],\n",
        "                        \"width\": base_image.get(\"width\"),\n",
        "                        \"height\": base_image.get(\"height\")\n",
        "                    }\n",
        "\n",
        "                    image_placeholders[image_id] = image_metadata\n",
        "\n",
        "                if image_placeholders:\n",
        "                    new_metadata = doc.metadata.copy()\n",
        "                    new_metadata[\"images\"] = image_placeholders\n",
        "                    enhanced_doc = Document(\n",
        "                        page_content=doc.page_content,\n",
        "                        metadata=new_metadata\n",
        "                    )\n",
        "                    enhanced_docs.append(enhanced_doc)\n",
        "                else:\n",
        "                    enhanced_docs.append(doc)\n",
        "\n",
        "        pdf_document.close()\n",
        "        return enhanced_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpPh62NxE0eS",
        "outputId": "391d9458-4023-4d66-9082-9733bc6804a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 82 chunks, 52 contain images\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = EnhancedPyMuPDFLoader(\"/content/GPU.pdf\")\n",
        "documents = loader.load()\n",
        "\n",
        "class ImageAwareTextSplitter(RecursiveCharacterTextSplitter):\n",
        "    def split_documents(self, documents):\n",
        "        \"\"\"Split documents while preserving image metadata\"\"\"\n",
        "        splits = super().split_documents(documents)\n",
        "\n",
        "        for split in splits:\n",
        "            parent_metadata = next((doc.metadata for doc in documents\n",
        "                                    if doc.metadata.get('page') == split.metadata.get('page')), {})\n",
        "\n",
        "            parent_images = parent_metadata.get('images', {})\n",
        "            if not parent_images:\n",
        "                continue\n",
        "\n",
        "            chunk_images = {}\n",
        "            for img_id, img_data in parent_images.items():\n",
        "                if f\"[IMAGE: {img_id}]\" in split.page_content:\n",
        "                    chunk_images[img_id] = img_data\n",
        "\n",
        "            if chunk_images:\n",
        "                split.metadata[\"images\"] = chunk_images\n",
        "\n",
        "        return splits\n",
        "\n",
        "\n",
        "splitter = ImageAwareTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = splitter.split_documents(documents)\n",
        "image_chunks = [chunk for chunk in chunks if \"images\" in chunk.metadata]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qeZyuJv_LTmd"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "for chunk in image_chunks:\n",
        "    if \"images\" in chunk.metadata and isinstance(chunk.metadata[\"images\"], dict):\n",
        "        chunk.metadata[\"images\"] = json.dumps(chunk.metadata[\"images\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yzWKZrLoH4qt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73f59c3d-cbb4-41b4-96d7-3259cfecd9cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-f8eb87f20175>:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L12-v2\",\n",
        "    model_kwargs={'device': 'cpu'}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpEgNWVqIe7J",
        "outputId": "8881d343-d42d-4ca7-dbb3-1eff9fd0e7c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 'multiagent-rag' already exists and has been initialized\n",
            "Added 82 chunks and 52 image chunks to the vector store\n"
          ]
        }
      ],
      "source": [
        "# Create Pinecone vector store\n",
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from google.colab import userdata\n",
        "\n",
        "pc = Pinecone(api_key=userdata.get(\"PINECONE_API_KEY\"))\n",
        "\n",
        "index_name = \"multiagent-rag\"\n",
        "\n",
        "# Check if index already exists\n",
        "if index_name not in [index['name'] for index in pc.list_indexes()]:\n",
        "    # Create index if it doesn't exist\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=384,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(\n",
        "            cloud=\"aws\",\n",
        "            region=\"us-east-1\"\n",
        "        )\n",
        "    )\n",
        "    print(f\"Index '{index_name}' created.\")\n",
        "    pinecone_index = pc.Index(index_name)\n",
        "else:\n",
        "    pinecone_index = pc.Index(index_name)\n",
        "    print(f\"Index '{index_name}' already exists and has been initialized\")\n",
        "\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=pinecone_index,embedding=embedding_model)\n",
        "vector_store.add_documents(chunks)\n",
        "# vector_store.add_documents(image_chunks)\n",
        "print(f\"Added {len(chunks)} chunks and {len(image_chunks)} image chunks to the vector store\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "71d9I4CAGmiL"
      },
      "outputs": [],
      "source": [
        "class RetrievalAgent:\n",
        "    def __init__(self):\n",
        "        self.embeddings = embedding_model\n",
        "        self.vector_store = vector_store\n",
        "\n",
        "    def retrieve(self, query, k=3):\n",
        "        \"\"\"Retrieve the top-k most relevant documents for the query\"\"\"\n",
        "        docs_with_scores = self.vector_store.similarity_search_with_score(query, k=k)\n",
        "        return docs_with_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8iw1XOIRHTm0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "class ImageAgent:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def identify_images(self, retrieved_docs_with_scores):\n",
        "        \"\"\"\n",
        "        Identify images in the retrieved documents\n",
        "        Returns: List of image information dictionaries\n",
        "        \"\"\"\n",
        "        images_info = []\n",
        "\n",
        "        for doc, score in retrieved_docs_with_scores:\n",
        "            doc_images_raw = doc.metadata.get(\"images\", {})\n",
        "            if isinstance(doc_images_raw, str):\n",
        "                try:\n",
        "                    doc_images = json.loads(doc_images_raw)\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Warning: Could not decode image metadata for document on page {doc.metadata.get('page')}. Skipping images for this document.\")\n",
        "                    continue\n",
        "            elif isinstance(doc_images_raw, dict):\n",
        "                doc_images = doc_images_raw\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            if not isinstance(doc_images, dict):\n",
        "                continue\n",
        "\n",
        "            for img_id, img_data in doc_images.items():\n",
        "                image_info = img_data.copy()\n",
        "                placeholder = f\"[IMAGE: {img_id}]\"\n",
        "                if placeholder in doc.page_content:\n",
        "                    placeholder_pos = doc.page_content.find(placeholder)\n",
        "                    start_pos = max(0, placeholder_pos - 100)\n",
        "                    end_pos = min(len(doc.page_content), placeholder_pos + len(placeholder) + 100)\n",
        "                    image_info[\"document_context\"] = doc.page_content[start_pos:end_pos]\n",
        "                else:\n",
        "                    image_info[\"document_context\"] = \"Context not available\"\n",
        "\n",
        "                image_info[\"relevance_score\"] = score\n",
        "                image_info[\"placeholder\"] = placeholder\n",
        "\n",
        "                images_info.append(image_info)\n",
        "\n",
        "        return images_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eSiabWtoHeJ-"
      },
      "outputs": [],
      "source": [
        "class SimpleOrchestrator:\n",
        "    def __init__(self, retrieval_agent, image_agent):\n",
        "        \"\"\"Initialize the orchestrator\"\"\"\n",
        "        self.retrieval_agent = retrieval_agent\n",
        "        self.image_agent = image_agent\n",
        "\n",
        "    def process_query(self, query, k=3):\n",
        "        \"\"\"Process a user query through the pipeline\"\"\"\n",
        "        retrieved_docs_with_scores = self.retrieval_agent.retrieve(query, k=k)\n",
        "        image_info = self.image_agent.identify_images(retrieved_docs_with_scores)\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"retrieved_docs_with_scores\": retrieved_docs_with_scores,\n",
        "            \"image_info\": image_info\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "taLjk_vlHga7"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from google.colab import userdata\n",
        "\n",
        "class EnhancedResponseGenerator:\n",
        "    def __init__(self, orchestrator):\n",
        "        self.orchestrator = orchestrator\n",
        "\n",
        "        self.llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.5-flash-preview-04-17\",api_key=userdata.get(\"GOOGLE_API_KEY_1\"))\n",
        "\n",
        "        # Create prompt template with image information\n",
        "        self.prompt_template = PromptTemplate.from_template(\n",
        "            \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            Images in the context:\n",
        "            {image_info}\n",
        "\n",
        "            Question: {query}\n",
        "\n",
        "            Please provide a detailed and accurate answer based only on the information in the context.\n",
        "            When there are images mentioned in the text, acknowledge them like \"[There is an image here showing X]\"\n",
        "            based on the surrounding context.\n",
        "            If the context doesn't contain relevant information to answer the question, say so.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    def generate_response(self, query):\n",
        "        results = self.orchestrator.process_query(query)\n",
        "        context = \"\\n\\n\".join([\n",
        "            f\"Document (relevance: {score:.2f}):\\n{doc.page_content}\"\n",
        "            for doc, score in results[\"retrieved_docs_with_scores\"]\n",
        "        ])\n",
        "\n",
        "        image_info = \"\"\n",
        "        if results[\"image_info\"]:\n",
        "            for i, img in enumerate(results[\"image_info\"]):\n",
        "                image_info += f\"Image {i+1}: {img['placeholder']} (on page {img.get('page_num', 'unknown')})\\n\"\n",
        "                image_info += f\"Context around image: {img.get('document_context', 'Not available')}\\n\\n\"\n",
        "        else:\n",
        "            image_info = \"No images found in the retrieved documents.\"\n",
        "\n",
        "        response = self.llm.invoke(\n",
        "            self.prompt_template.format(\n",
        "                query=query,\n",
        "                context=context,\n",
        "                image_info=image_info\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD-WhvXKHuCS",
        "outputId": "b75f9381-4ad2-4a9e-99c3-78751e37d21d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context:\n",
            "\n",
            "A General Purpose Graphics Processing Unit (GPGPU) is a modified GPU designed for graphics rendering but adapted to perform applications traditionally handled by CPUs, such as floating-point calculations. Essentially, all modern GPUs are considered GPGPUs. They can be programmed to deploy their processing power for scientific computing needs.\n",
            "\n",
            "Regarding architecture and memory, the context states:\n",
            "*   GPGPUs are connected to a CPU. The CPU offloads compute-heavy jobs to the GPU in terms of concurrent instruction streams.\n",
            "*   GPUs have more transistors dedicated to data processing than to control or caches.\n",
            "*   The L1 cache in GPUs is small, and in earlier versions, it was absent.\n",
            "*   The context mentions that a more detailed discussion on GPU memories will be presented later while discussing CUDA.\n",
            "\n",
            "[There is an image here, but the context around it is not available to describe what it shows.]\n",
            "Found 1 images in the retrieved documents\n",
            "Image ID: img_8_1\n",
            "Context: Context not available...\n"
          ]
        }
      ],
      "source": [
        "# Initialize all components\n",
        "retrieval_agent = RetrievalAgent()\n",
        "image_agent = ImageAgent()\n",
        "orchestrator = SimpleOrchestrator(retrieval_agent, image_agent)\n",
        "generator = EnhancedResponseGenerator(orchestrator)\n",
        "\n",
        "question = \"Explain me GPGPU architecture and memory\"\n",
        "response = generator.generate_response(question)\n",
        "print(response)\n",
        "\n",
        "results = orchestrator.process_query(question)\n",
        "print(f\"Found {len(results['image_info'])} images in the retrieved documents\")\n",
        "for img in results['image_info']:\n",
        "    print(f\"Image ID: {img['image_id']}\")\n",
        "    print(f\"Context: {img['document_context'][:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACTPKi2cH0as"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}