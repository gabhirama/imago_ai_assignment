{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_PXHr3eE4SF",
        "outputId": "3cad9149-8d4e-407e-d328-a5bad06e9934"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: pinecone 6.0.2 does not provide the extra 'async'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.9/437.9 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.9/421.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet pymupdf langchain langchain-core langchain_huggingface langchain-google-genai langchain-pinecone langchain-community sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyPfLt0rEsnI"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "class EnhancedPyMuPDFLoader(PyMuPDFLoader):\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\"Load documents and detect images\"\"\"\n",
        "        docs = super().load()\n",
        "        enhanced_docs = []\n",
        "\n",
        "        pdf_document = fitz.open(self.file_path)\n",
        "        for i, page in enumerate(pdf_document):\n",
        "            page_docs = [doc for doc in docs if doc.metadata.get('page') == i]\n",
        "\n",
        "            image_list = page.get_images(full=True)\n",
        "\n",
        "            for doc in page_docs:\n",
        "                image_placeholders = {}\n",
        "\n",
        "                for img_idx, img_info in enumerate(image_list):\n",
        "                    xref = img_info[0]\n",
        "\n",
        "                    base_image = pdf_document.extract_image(xref)\n",
        "\n",
        "                    image_id = f\"img_{i+1}_{img_idx+1}\"\n",
        "                    placeholder = f\"[IMAGE: {image_id}]\"\n",
        "                    image_metadata = {\n",
        "                        \"image_id\": image_id,\n",
        "                        \"page_num\": i + 1,\n",
        "                        \"position\": img_idx + 1,\n",
        "                        \"source_file\": self.file_path,\n",
        "                        \"image_format\": base_image[\"ext\"],\n",
        "                        \"width\": base_image.get(\"width\"),\n",
        "                        \"height\": base_image.get(\"height\")\n",
        "                    }\n",
        "\n",
        "                    image_placeholders[image_id] = image_metadata\n",
        "\n",
        "                if image_placeholders:\n",
        "                    new_metadata = doc.metadata.copy()\n",
        "                    new_metadata[\"images\"] = image_placeholders\n",
        "                    enhanced_doc = Document(\n",
        "                        page_content=doc.page_content,\n",
        "                        metadata=new_metadata\n",
        "                    )\n",
        "                    enhanced_docs.append(enhanced_doc)\n",
        "                else:\n",
        "                    enhanced_docs.append(doc)\n",
        "\n",
        "        pdf_document.close()\n",
        "        return enhanced_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "rpPh62NxE0eS"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = EnhancedPyMuPDFLoader(\"/content/GPU.pdf\")\n",
        "documents = loader.load()\n",
        "\n",
        "class ImageAwareTextSplitter(RecursiveCharacterTextSplitter):\n",
        "    def split_documents(self, documents):\n",
        "        \"\"\"Split documents while preserving image metadata\"\"\"\n",
        "        splits = super().split_documents(documents)\n",
        "\n",
        "        for split in splits:\n",
        "            parent_metadata = next((doc.metadata for doc in documents\n",
        "                                    if doc.metadata.get('page') == split.metadata.get('page')), {})\n",
        "\n",
        "            parent_images = parent_metadata.get('images', {})\n",
        "            if not parent_images:\n",
        "                continue\n",
        "\n",
        "            chunk_images = {}\n",
        "            for img_id, img_data in parent_images.items():\n",
        "                if f\"[IMAGE: {img_id}]\" in split.page_content:\n",
        "                    chunk_images[img_id] = img_data\n",
        "\n",
        "            if chunk_images:\n",
        "                split.metadata[\"images\"] = chunk_images\n",
        "\n",
        "        return splits\n",
        "\n",
        "\n",
        "splitter = ImageAwareTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = splitter.split_documents(documents)\n",
        "image_chunks = [chunk for chunk in chunks if \"images\" in chunk.metadata]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "qeZyuJv_LTmd"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "for chunk in image_chunks:\n",
        "    if \"images\" in chunk.metadata and isinstance(chunk.metadata[\"images\"], dict):\n",
        "        chunk.metadata[\"images\"] = json.dumps(chunk.metadata[\"images\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "yzWKZrLoH4qt"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"intfloat/multilingual-e5-large-instruct\",\n",
        "    model_kwargs={'device': 'cpu'}\n",
        ")\n",
        "\n",
        "sample_embedding = embedding_model.embed_query(\"test\")\n",
        "embedding_dimensions = len(sample_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2Zs6PML4-Q9",
        "outputId": "8a90e1f8-13b8-4989-b568-3dc4c1ad3dce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index 'multiagent-rag' already exists and has been initialized\n"
          ]
        }
      ],
      "source": [
        "# Create Pinecone vector store\n",
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from google.colab import userdata\n",
        "\n",
        "pc = Pinecone(api_key=userdata.get(\"PINECONE_API_KEY\"))\n",
        "\n",
        "index_name = \"multiagent-rag\"\n",
        "\n",
        "# Check if index already exists\n",
        "if index_name not in [index['name'] for index in pc.list_indexes()]:\n",
        "    # Create index if it doesn't exist\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=embedding_dimensions,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(\n",
        "            cloud=\"aws\",\n",
        "            region=\"us-east-1\"\n",
        "        )\n",
        "    )\n",
        "    print(f\"Index '{index_name}' created.\")\n",
        "    pinecone_index = pc.Index(index_name)\n",
        "else:\n",
        "    pinecone_index = pc.Index(index_name)\n",
        "    print(f\"Index '{index_name}' already exists and has been initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "feF09jlD5Cjy"
      },
      "outputs": [],
      "source": [
        "# Code to delete an index to create a new one\n",
        "# pc.delete_index(\"multiagent-rag\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpEgNWVqIe7J",
        "outputId": "206dfb72-dc26-4275-aab2-a1588e9f6123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 82 chunks and 52 image chunks to the vector store\n"
          ]
        }
      ],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=pinecone_index,embedding=embedding_model)\n",
        "vector_store.add_documents(chunks)\n",
        "# vector_store.add_documents(image_chunks)\n",
        "print(f\"Added {len(chunks)} chunks and {len(image_chunks)} image chunks to the vector store\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "71d9I4CAGmiL"
      },
      "outputs": [],
      "source": [
        "class RetrievalAgent:\n",
        "    def __init__(self):\n",
        "        self.embeddings = embedding_model\n",
        "        self.vector_store = vector_store\n",
        "\n",
        "    def retrieve(self, query, k=3):\n",
        "        \"\"\"Retrieve the top-k most relevant documents for the query\"\"\"\n",
        "        docs_with_scores = self.vector_store.similarity_search_with_score(query, k=k)\n",
        "        return docs_with_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "8iw1XOIRHTm0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "class ImageAgent:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def identify_images(self, retrieved_docs_with_scores):\n",
        "        \"\"\"\n",
        "        Identify images in the retrieved documents\n",
        "        Returns: List of image information dictionaries\n",
        "        \"\"\"\n",
        "        images_info = []\n",
        "\n",
        "        for doc, score in retrieved_docs_with_scores:\n",
        "            doc_images_raw = doc.metadata.get(\"images\", {})\n",
        "            if isinstance(doc_images_raw, str):\n",
        "                try:\n",
        "                    doc_images = json.loads(doc_images_raw)\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Warning: Could not decode image metadata for document on page {doc.metadata.get('page')}. Skipping images for this document.\")\n",
        "                    continue\n",
        "            elif isinstance(doc_images_raw, dict):\n",
        "                doc_images = doc_images_raw\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            if not isinstance(doc_images, dict):\n",
        "                continue\n",
        "\n",
        "            for img_id, img_data in doc_images.items():\n",
        "                image_info = img_data.copy()\n",
        "                placeholder = f\"[IMAGE: {img_id}]\"\n",
        "                if placeholder in doc.page_content:\n",
        "                    placeholder_pos = doc.page_content.find(placeholder)\n",
        "                    start_pos = max(0, placeholder_pos - 100)\n",
        "                    end_pos = min(len(doc.page_content), placeholder_pos + len(placeholder) + 100)\n",
        "                    image_info[\"document_context\"] = doc.page_content[start_pos:end_pos]\n",
        "                else:\n",
        "                    image_info[\"document_context\"] = \"Context not available\"\n",
        "\n",
        "                image_info[\"relevance_score\"] = score\n",
        "                image_info[\"placeholder\"] = placeholder\n",
        "\n",
        "                images_info.append(image_info)\n",
        "\n",
        "        return images_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "eSiabWtoHeJ-"
      },
      "outputs": [],
      "source": [
        "class SimpleOrchestrator:\n",
        "    def __init__(self, retrieval_agent, image_agent):\n",
        "        \"\"\"Initialize the orchestrator\"\"\"\n",
        "        self.retrieval_agent = retrieval_agent\n",
        "        self.image_agent = image_agent\n",
        "\n",
        "    def process_query(self, query, k=3):\n",
        "        \"\"\"Process a user query through the pipeline\"\"\"\n",
        "        retrieved_docs_with_scores = self.retrieval_agent.retrieve(query, k=k)\n",
        "        image_info = self.image_agent.identify_images(retrieved_docs_with_scores)\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"retrieved_docs_with_scores\": retrieved_docs_with_scores,\n",
        "            \"image_info\": image_info\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "taLjk_vlHga7"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from google.colab import userdata\n",
        "\n",
        "class EnhancedResponseGenerator:\n",
        "    def __init__(self, orchestrator):\n",
        "        self.orchestrator = orchestrator\n",
        "\n",
        "        self.llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.5-flash-preview-04-17\",api_key=userdata.get(\"GOOGLE_API_KEY_1\"))\n",
        "\n",
        "        # Create prompt template with image information\n",
        "        self.prompt_template = PromptTemplate.from_template(\n",
        "            \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            Images in the context:\n",
        "            {image_info}\n",
        "\n",
        "            Question: {query}\n",
        "\n",
        "            Please provide a detailed and accurate answer based only on the information in the context.\n",
        "            When there are images mentioned in the text, acknowledge them like \"[There is an image here showing X]\"\n",
        "            based on the surrounding context.\n",
        "            If the context doesn't contain relevant information to answer the question, say so.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    def generate_response(self, query):\n",
        "        results = self.orchestrator.process_query(query)\n",
        "        context = \"\\n\\n\".join([\n",
        "            f\"Document (relevance: {score:.2f}):\\n{doc.page_content}\"\n",
        "            for doc, score in results[\"retrieved_docs_with_scores\"]\n",
        "        ])\n",
        "\n",
        "        image_info = \"\"\n",
        "        if results[\"image_info\"]:\n",
        "            for i, img in enumerate(results[\"image_info\"]):\n",
        "                image_info += f\"Image {i+1}: {img['placeholder']} (on page {img.get('page_num', 'unknown')})\\n\"\n",
        "                image_info += f\"Context around image: {img.get('document_context', 'Not available')}\\n\\n\"\n",
        "        else:\n",
        "            image_info = \"No images found in the retrieved documents.\"\n",
        "\n",
        "        response = self.llm.invoke(\n",
        "            self.prompt_template.format(\n",
        "                query=query,\n",
        "                context=context,\n",
        "                image_info=image_info\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD-WhvXKHuCS",
        "outputId": "8d0d3c20-a7e8-46c8-9669-18fa238bf722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the context provided:\n",
            "\n",
            "Kernels are defined as functions that are called from the CPU code and executed in the GPU. Kernels return void. Different kernels can be launched with different execution instructions. During kernel execution, the GPU can access memory from DRAM.\n",
            "Found 2 images in the retrieved documents\n",
            "Image ID: img_46_1\n",
            "Context: Context not available...\n",
            "Image ID: img_46_2\n",
            "Context: Context not available...\n"
          ]
        }
      ],
      "source": [
        "# Initialize all components\n",
        "retrieval_agent = RetrievalAgent()\n",
        "image_agent = ImageAgent()\n",
        "orchestrator = SimpleOrchestrator(retrieval_agent, image_agent)\n",
        "generator = EnhancedResponseGenerator(orchestrator)\n",
        "\n",
        "question = \"How do Kernels work?\"\n",
        "response = generator.generate_response(question)\n",
        "print(response)\n",
        "\n",
        "results = orchestrator.process_query(question)\n",
        "print(f\"Found {len(results['image_info'])} images in the retrieved documents\")\n",
        "for img in results['image_info']:\n",
        "    print(f\"Image ID: {img['image_id']}\")\n",
        "    print(f\"Context: {img['document_context']}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODsnkqTdAavH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
