{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kp5wYVNoOtk5",
        "outputId": "55a3c9d2-54fb-4738-f38b-908c2f62b73b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: pinecone 6.0.2 does not provide the extra 'async'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.9/437.9 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.9/421.9 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet pymupdf pillow pytesseract langchain langchain-core langchain_huggingface langchain-google-genai langchain-pinecone langchain-community sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain_core.documents import Document\n",
        "import os\n",
        "import json # For metadata serialization\n",
        "\n",
        "class EnhancedPyMuPDFLoader(PyMuPDFLoader):\n",
        "    def load(self):\n",
        "        \"\"\"Load documents, extract text, insert image placeholders, and store image metadata.\"\"\"\n",
        "        docs_from_super = super().load()  # Provides one Document per page\n",
        "\n",
        "        processed_docs = []\n",
        "        pdf_document = fitz.open(self.file_path)\n",
        "\n",
        "        if len(docs_from_super) != len(pdf_document):\n",
        "            print(f\"Warning: Mismatch in page count between Langchain loader ({len(docs_from_super)}) and fitz ({len(pdf_document)}). Processing based on fitz page count if super().load() is shorter.\")\n",
        "\n",
        "        for i, page_fitz in enumerate(pdf_document):\n",
        "            # Try to get the corresponding doc from super().load()\n",
        "            page_doc_from_super = None\n",
        "            if i < len(docs_from_super) and docs_from_super[i].metadata.get('page') == i:\n",
        "                page_doc_from_super = docs_from_super[i]\n",
        "\n",
        "            current_page_content = page_doc_from_super.page_content if page_doc_from_super else page_fitz.get_text(\"text\", sort=True)\n",
        "            page_metadata = page_doc_from_super.metadata.copy() if page_doc_from_super else {\"source\": self.file_path, \"page\": i}\n",
        "\n",
        "            image_list = page_fitz.get_images(full=True)\n",
        "            page_specific_image_metadata = {}\n",
        "            appended_placeholders_text = []\n",
        "\n",
        "            for img_idx, img_info in enumerate(image_list):\n",
        "                xref = img_info[0]\n",
        "                # Create a unique image ID: p{page_num_1_indexed}_i{image_index_1_indexed}\n",
        "                image_id = f\"img_p{i+1}_i{img_idx+1}\"\n",
        "                placeholder = f\"[IMAGE: {image_id}]\"\n",
        "\n",
        "                appended_placeholders_text.append(placeholder)\n",
        "\n",
        "                img_meta = {\n",
        "                    \"image_id\": image_id,\n",
        "                    \"page_num\": i + 1,  # 1-indexed for user display\n",
        "                    \"image_index_on_page\": img_idx + 1,\n",
        "                    \"xref\": xref,  # Crucial for later extraction\n",
        "                    \"source_file\": self.file_path\n",
        "                    # Dimensions and format will be added by ImageAnalysisAgent\n",
        "                }\n",
        "                page_specific_image_metadata[image_id] = img_meta\n",
        "\n",
        "            if appended_placeholders_text:\n",
        "                # Append placeholders at the end of the page content\n",
        "                current_page_content += \"\\n\\nImage Placeholders on this page:\\n\" + \"\\n\".join(appended_placeholders_text)\n",
        "\n",
        "            if page_specific_image_metadata:\n",
        "                # Store metadata for all images on this page\n",
        "                page_metadata[\"images_on_page\"] = page_specific_image_metadata\n",
        "\n",
        "            processed_docs.append(Document(page_content=current_page_content, metadata=page_metadata))\n",
        "\n",
        "        pdf_document.close()\n",
        "        return processed_docs"
      ],
      "metadata": {
        "id": "dskF0xnoPaha"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import json # Ensure json is imported here\n",
        "\n",
        "class ImageAwareTextSplitter(RecursiveCharacterTextSplitter):\n",
        "    def split_documents(self, documents): # documents are page-level from EnhancedPyMuPDFLoader\n",
        "        \"\"\"Split documents while preserving image metadata for relevant chunks.\"\"\"\n",
        "        splits = super().split_documents(documents)\n",
        "\n",
        "        for split in splits:\n",
        "            # Find the original page document that this split came from\n",
        "            # RecursiveCharacterTextSplitter preserves metadata from the parent document,\n",
        "            # including 'page' and 'images_on_page' if they were there.\n",
        "            original_doc_metadata = split.metadata\n",
        "\n",
        "            images_on_original_page = original_doc_metadata.get('images_on_page', {})\n",
        "\n",
        "            if images_on_original_page:\n",
        "                chunk_specific_images = {}\n",
        "                for img_id, img_data_from_loader in images_on_original_page.items():\n",
        "                    # If the placeholder (which was added to page_content by the loader)\n",
        "                    # is present in this specific text chunk, then this image is relevant to this chunk.\n",
        "                    if f\"[IMAGE: {img_id}]\" in split.page_content:\n",
        "                        # Ensure img_data_from_loader is a dictionary before copying\n",
        "                        if isinstance(img_data_from_loader, dict):\n",
        "                            chunk_specific_images[img_id] = img_data_from_loader.copy()\n",
        "                        else:\n",
        "                             print(f\"Warning: Expected dictionary for image metadata {img_id}, but got {type(img_data_from_loader)}. Skipping.\")\n",
        "\n",
        "                if chunk_specific_images:\n",
        "                    # Store image metadata relevant *to this chunk* in split.metadata[\"images\"]\n",
        "                    # This \"images\" key will be used by the ImageAgent later.\n",
        "                    # It will be serialized to JSON string before storing in vector DB.\n",
        "                    split.metadata[\"images\"] = chunk_specific_images # Store as dict here\n",
        "\n",
        "            # **FIX:** Explicitly remove the original 'images_on_page' key from the chunk's metadata\n",
        "            # This is crucial because Pinecone does not accept the dictionary format.\n",
        "            if \"images_on_page\" in split.metadata:\n",
        "                del split.metadata[\"images_on_page\"]\n",
        "\n",
        "        return splits"
      ],
      "metadata": {
        "id": "pqB8NbfSPhQr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "from PIL import Image as PILImage\n",
        "import io\n",
        "import fitz # PyMuPDF\n",
        "\n",
        "# For Colab/Linux, Tesseract is usually in path. For others, you might need:\n",
        "# pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract' # Example\n",
        "\n",
        "class ImageAnalysisAgent:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def _extract_image_details_from_pdf(self, pdf_path, xref):\n",
        "        \"\"\"Helper to extract image bytes, format, width, height from PDF using xref.\"\"\"\n",
        "        doc = None\n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            base_image = doc.extract_image(xref) # xref is int\n",
        "            if not base_image:\n",
        "                return None, None, None, None, \"Extraction failed\"\n",
        "\n",
        "            image_bytes = base_image[\"image\"]\n",
        "            image_format = base_image[\"ext\"]\n",
        "            width = base_image.get(\"width\")\n",
        "            height = base_image.get(\"height\")\n",
        "            return image_bytes, image_format, width, height, None\n",
        "        except Exception as e:\n",
        "            error_message = f\"Error extracting image with xref {xref} from {pdf_path}: {e}\"\n",
        "            print(error_message)\n",
        "            return None, None, None, None, error_message\n",
        "        finally:\n",
        "            if doc:\n",
        "                doc.close()\n",
        "\n",
        "    def analyze_images(self, image_infos_list):\n",
        "        \"\"\"\n",
        "        Analyzes a list of image metadata dictionaries.\n",
        "        Each dict in image_infos_list is expected to have 'source_file', 'xref', 'image_id'.\n",
        "        It adds 'ocr_text', 'image_format', 'width', 'height', 'image_type', 'calculated_relevance'.\n",
        "        'image_infos_list' comes from ImageAgent.identify_images.\n",
        "        \"\"\"\n",
        "        analyzed_results = []\n",
        "\n",
        "        for img_info_from_agent in image_infos_list:\n",
        "            # img_info_from_agent already contains: image_id, page_num, xref, source_file,\n",
        "            # document_context (text around placeholder), relevance_score (from vector search), placeholder.\n",
        "            analyzed_img_data = img_info_from_agent.copy()\n",
        "\n",
        "            pdf_path = analyzed_img_data.get(\"source_file\")\n",
        "            xref = analyzed_img_data.get(\"xref\")\n",
        "\n",
        "            if not pdf_path or xref is None:\n",
        "                print(f\"Warning: Missing source_file or xref for image_id {analyzed_img_data.get('image_id')}. Skipping analysis.\")\n",
        "                analyzed_img_data.update({\n",
        "                    \"ocr_text\": None, \"image_format\": None, \"width\": None, \"height\": None,\n",
        "                    \"image_type\": \"unknown\", \"calculated_relevance\": 0.0, \"analysis_error\": \"Missing source/xref\"\n",
        "                })\n",
        "                analyzed_results.append(analyzed_img_data)\n",
        "                continue\n",
        "\n",
        "            image_bytes, img_format, width, height, error = self._extract_image_details_from_pdf(pdf_path, xref)\n",
        "\n",
        "            analyzed_img_data[\"image_format\"] = img_format\n",
        "            analyzed_img_data[\"width\"] = width\n",
        "            analyzed_img_data[\"height\"] = height\n",
        "            analyzed_img_data[\"analysis_error\"] = error\n",
        "\n",
        "            ocr_text = None\n",
        "            if image_bytes and not error:\n",
        "                try:\n",
        "                    pil_img = PILImage.open(io.BytesIO(image_bytes))\n",
        "                    ocr_text = pytesseract.image_to_string(pil_img)\n",
        "                    ocr_text = ocr_text.strip() if ocr_text else None\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during OCR for image {analyzed_img_data.get('image_id')}: {e}\")\n",
        "                    analyzed_img_data[\"analysis_error\"] = (analyzed_img_data[\"analysis_error\"] or \"\") + f\"; OCR Error: {e}\"\n",
        "                    ocr_text = None\n",
        "            analyzed_img_data[\"ocr_text\"] = ocr_text\n",
        "\n",
        "            # Image Type (Heuristic based on format)\n",
        "            if img_format:\n",
        "                fmt_lower = img_format.lower()\n",
        "                if fmt_lower in [\"jpeg\", \"jpg\", \"png\", \"gif\", \"bmp\"]:\n",
        "                    analyzed_img_data[\"image_type\"] = \"raster_graphic\" # Could be photo, diagram, screenshot\n",
        "                elif fmt_lower == \"tiff\":\n",
        "                    analyzed_img_data[\"image_type\"] = \"tagged_image_file_format\" # Often scans or high quality\n",
        "                elif fmt_lower == \"svg\":\n",
        "                     analyzed_img_data[\"image_type\"] = \"vector_graphic\"\n",
        "                else:\n",
        "                    analyzed_img_data[\"image_type\"] = img_format\n",
        "            else:\n",
        "                analyzed_img_data[\"image_type\"] = \"unknown\"\n",
        "\n",
        "            # Basic Relevance Score (heuristic: larger images or images with some OCR text are more relevant)\n",
        "            # This is a calculated score, distinct from the vector search relevance_score.\n",
        "            calculated_relevance = 0.0\n",
        "            if width and height and width > 50 and height > 50: # Arbitrary size threshold\n",
        "                calculated_relevance += 0.4\n",
        "            if ocr_text and len(ocr_text) > 10: # Arbitrary OCR text length threshold\n",
        "                calculated_relevance += 0.6\n",
        "            analyzed_img_data[\"calculated_relevance\"] = min(1.0, calculated_relevance) # Cap at 1.0\n",
        "\n",
        "            analyzed_results.append(analyzed_img_data)\n",
        "\n",
        "        return analyzed_results"
      ],
      "metadata": {
        "id": "Ntt6BXNlPjkp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json # Ensure this is imported in the cell with ImageAgent\n",
        "\n",
        "class ImageAgent:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def identify_images(self, retrieved_docs_with_scores):\n",
        "        \"\"\"\n",
        "        Identify images in the retrieved document chunks.\n",
        "        Returns: List of image information dictionaries, ready for deeper analysis.\n",
        "        Each dict contains metadata from the chunk, context, and vector search score.\n",
        "        \"\"\"\n",
        "        images_info_for_analysis = []\n",
        "\n",
        "        for doc_chunk, score in retrieved_docs_with_scores: # doc_chunk is a text split\n",
        "            # 'images' metadata in the chunk was populated by ImageAwareTextSplitter\n",
        "            # and potentially JSON stringified before storing in vector DB\n",
        "            chunk_images_metadata_raw = doc_chunk.metadata.get(\"images\", {})\n",
        "\n",
        "            parsed_chunk_images_metadata = {}\n",
        "            if isinstance(chunk_images_metadata_raw, str):\n",
        "                try:\n",
        "                    parsed_chunk_images_metadata = json.loads(chunk_images_metadata_raw)\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Warning: Could not decode image metadata string for a chunk from page {doc_chunk.metadata.get('page')}.\")\n",
        "                    # parsed_chunk_images_metadata remains {}\n",
        "            elif isinstance(chunk_images_metadata_raw, dict):\n",
        "                parsed_chunk_images_metadata = chunk_images_metadata_raw\n",
        "\n",
        "            if not isinstance(parsed_chunk_images_metadata, dict): # Should be a dict by now\n",
        "                continue\n",
        "\n",
        "            for img_id, img_data_from_splitter in parsed_chunk_images_metadata.items():\n",
        "                # img_data_from_splitter contains: image_id, page_num, image_index_on_page, xref, source_file\n",
        "\n",
        "                # Create a new dict for this image, copying essential info\n",
        "                current_image_info = img_data_from_splitter.copy()\n",
        "\n",
        "                placeholder = f\"[IMAGE: {img_id}]\" # Reconstruct placeholder to be sure\n",
        "                current_image_info[\"placeholder\"] = placeholder\n",
        "\n",
        "                # Extract text context around the placeholder from the chunk's content\n",
        "                if placeholder in doc_chunk.page_content:\n",
        "                    placeholder_pos = doc_chunk.page_content.find(placeholder)\n",
        "                    start_pos = max(0, placeholder_pos - 150) # More context\n",
        "                    end_pos = min(len(doc_chunk.page_content), placeholder_pos + len(placeholder) + 150)\n",
        "                    current_image_info[\"document_context\"] = doc_chunk.page_content[start_pos:end_pos]\n",
        "                else:\n",
        "                    # This case should ideally not happen if ImageAwareTextSplitter worked correctly\n",
        "                    # and placeholder was indeed in the chunk that references this image.\n",
        "                    current_image_info[\"document_context\"] = \"Placeholder not found in chunk context.\"\n",
        "\n",
        "                current_image_info[\"vector_search_relevance_score\"] = score # Relevance of the text chunk\n",
        "\n",
        "                images_info_for_analysis.append(current_image_info)\n",
        "\n",
        "        return images_info_for_analysis"
      ],
      "metadata": {
        "id": "eXUomI0fPlUK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Phase2Orchestrator:\n",
        "    def __init__(self, retrieval_agent, image_agent, image_analysis_agent):\n",
        "        self.retrieval_agent = retrieval_agent\n",
        "        self.image_agent = image_agent # Identifies images in retrieved docs + context\n",
        "        self.image_analysis_agent = image_analysis_agent # Performs OCR, etc.\n",
        "\n",
        "    def process_query(self, query, k=3):\n",
        "        # 1. Retrieve text chunks\n",
        "        retrieved_docs_with_scores = self.retrieval_agent.retrieve(query, k=k)\n",
        "\n",
        "        # 2. Identify image metadata associated with these chunks & get local text context\n",
        "        # Output: list of dicts, each is an image's info (incl. context, placeholder, xref, source_file, vector_score)\n",
        "        images_identified_in_chunks = self.image_agent.identify_images(retrieved_docs_with_scores)\n",
        "\n",
        "        # 3. Perform deeper analysis (OCR, type, etc.) on these identified images\n",
        "        analyzed_images_complete_info = []\n",
        "        if images_identified_in_chunks:\n",
        "             analyzed_images_complete_info = self.image_analysis_agent.analyze_images(images_identified_in_chunks)\n",
        "\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"retrieved_docs_with_scores\": retrieved_docs_with_scores, # List of (Document, score)\n",
        "            \"analyzed_image_info\": analyzed_images_complete_info # List of dicts with full analysis\n",
        "        }"
      ],
      "metadata": {
        "id": "IeMM_wK0Pm3h"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# (Existing imports)\n",
        "\n",
        "class EnhancedResponseGenerator:\n",
        "    def __init__(self, orchestrator): # orchestrator is now Phase2Orchestrator\n",
        "        self.orchestrator = orchestrator\n",
        "        # Ensure GOOGLE_API_KEY_1 is correctly loaded via userdata\n",
        "        self.llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", api_key=userdata.get(\"GOOGLE_API_KEY_1\"))\n",
        "\n",
        "        # Updated prompt template for Phase 2\n",
        "        self.prompt_template = PromptTemplate.from_template(\n",
        "            \"\"\"You are an AI assistant answering questions based on provided document excerpts and image analyses.\n",
        "Use only the information given in the 'Document Context' and 'Image Analysis Details'.\n",
        "\n",
        "Document Context:\n",
        "{context}\n",
        "\n",
        "Image Analysis Details:\n",
        "{image_analysis_details}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Based on all the provided information, answer the question.\n",
        "- If OCR text from an image is available and relevant, incorporate it into your answer.\n",
        "- If an image is described (e.g., type, context) but has no OCR, acknowledge its likely content based on the description if relevant.\n",
        "- Refer to images using their placeholders like \"[IMAGE: img_pX_iY]\" when discussing them.\n",
        "- If the information is insufficient to answer, clearly state that.\n",
        "Provide a comprehensive and accurate answer.\n",
        "\"\"\"\n",
        "        )\n",
        "\n",
        "    def generate_response(self, query):\n",
        "        # results will contain 'retrieved_docs_with_scores' and 'analyzed_image_info'\n",
        "        results = self.orchestrator.process_query(query)\n",
        "\n",
        "        # Format document context\n",
        "        context_str = \"\\n\\n---\\n\\n\".join([\n",
        "            f\"Excerpt from page {doc.metadata.get('page', 'N/A')+1} (Chunk Relevance: {score:.2f}):\\n{doc.page_content}\"\n",
        "            for doc, score in results[\"retrieved_docs_with_scores\"]\n",
        "        ])\n",
        "\n",
        "        # Format image analysis details\n",
        "        image_analysis_str = \"No specific images identified or analyzed for this query context.\\n\"\n",
        "        if results[\"analyzed_image_info\"]:\n",
        "            image_analysis_str = \"The following image analyses are relevant to the document context:\\n\"\n",
        "            for i, img_data in enumerate(results[\"analyzed_image_info\"]):\n",
        "                image_analysis_str += f\"\\n--- Image Analysis {i+1} ({img_data.get('placeholder', 'Unknown Placeholder')}) ---\\n\"\n",
        "                image_analysis_str += f\"  Source: Page {img_data.get('page_num', 'N/A')}, Image {img_data.get('image_index_on_page', 'N/A')} in PDF\\n\"\n",
        "                image_analysis_str += f\"  Detected Type: {img_data.get('image_type', 'N/A')}\\n\"\n",
        "                image_analysis_str += f\"  Dimensions: {img_data.get('width')}x{img_data.get('height')}px (Format: {img_data.get('image_format', 'N/A')})\\n\"\n",
        "                image_analysis_str += f\"  Calculated Image Relevance: {img_data.get('calculated_relevance', 0.0):.2f}\\n\"\n",
        "\n",
        "                ocr = img_data.get(\"ocr_text\")\n",
        "                if ocr:\n",
        "                    image_analysis_str += f\"  OCR Text from Image: \\\"{ocr[:700]}\\\"{(len(ocr)>700) * '...'}\\n\" # Truncate long OCR\n",
        "                else:\n",
        "                    image_analysis_str += \"  OCR Text from Image: Not available or not significant.\\n\"\n",
        "\n",
        "                image_analysis_str += f\"  Text Context from Document around image: \\\"...{img_data.get('document_context', 'N/A')}...\\\"\\n\"\n",
        "                if img_data.get(\"analysis_error\"):\n",
        "                     image_analysis_str += f\"  Analysis Note: {img_data.get('analysis_error')}\\n\"\n",
        "            image_analysis_str += \"\\n\"\n",
        "\n",
        "        # Prepare input for the LLM\n",
        "        llm_input_dict = {\n",
        "            \"query\": query,\n",
        "            \"context\": context_str,\n",
        "            \"image_analysis_details\": image_analysis_str\n",
        "        }\n",
        "\n",
        "        # Invoke the LLM\n",
        "        # The invoke method for ChatGoogleGenerativeAI expects a string, ChatPromptValue, or list of messages.\n",
        "        # Formatting the prompt template gives a string.\n",
        "        formatted_prompt = self.prompt_template.format(**llm_input_dict)\n",
        "        response = self.llm.invoke(formatted_prompt)\n",
        "\n",
        "        return response.content"
      ],
      "metadata": {
        "id": "LtC3lreSPorp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure all necessary imports are at the top of your notebook or script\n",
        "from google.colab import userdata # For API keys\n",
        "import json\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from langchain_pinecone import PineconeVectorStore"
      ],
      "metadata": {
        "id": "1fqcE0prQin5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 2: Define PDF Path and Load API Keys ---\n",
        "pdf_file_path = \"/content/GPU.pdf\" # Make sure this file exists\n",
        "PINECONE_API_KEY = userdata.get(\"PINECONE_API_KEY\")\n",
        "GOOGLE_API_KEY_1 = userdata.get(\"GOOGLE_API_KEY_1\") # Used in EnhancedResponseGenerator\n",
        "\n",
        "# --- Cell 3: Instantiate Components ---\n",
        "\n",
        "# 1. Enhanced Loader\n",
        "loader = EnhancedPyMuPDFLoader(pdf_file_path)\n",
        "documents = loader.load() # Page-level docs with 'images_on_page' metadata and placeholders in text\n",
        "\n",
        "# 2. Image-Aware Splitter\n",
        "splitter = ImageAwareTextSplitter(chunk_size=500, chunk_overlap=50, length_function=len)\n",
        "chunks = splitter.split_documents(documents) # Chunks with 'images' metadata (img_id -> {xref, source_file, ...})\n",
        "\n",
        "# 3. Serialize 'images' metadata for Pinecone (if it's a dictionary)\n",
        "for chunk in chunks:\n",
        "    # Check for the 'images' key (populated by the splitter) and ensure it's a dictionary\n",
        "    if \"images\" in chunk.metadata and isinstance(chunk.metadata[\"images\"], dict):\n",
        "        try:\n",
        "            # Serialize the dictionary to a JSON string\n",
        "            chunk.metadata[\"images\"] = json.dumps(chunk.metadata[\"images\"])\n",
        "        except TypeError as e:\n",
        "            print(f\"Error serializing image metadata for chunk on page {chunk.metadata.get('page')}: {e}. Removing problematic key.\")\n",
        "            # Remove the key if serialization fails to prevent the Pinecone error\n",
        "            del chunk.metadata[\"images\"]\n",
        "    # Ensure 'images_on_page' is NOT present in the metadata being sent to Pinecone\n",
        "    # This should be handled by the ImageAwareTextSplitter, but a final check doesn't hurt.\n",
        "    if \"images_on_page\" in chunk.metadata:\n",
        "         print(f\"Warning: 'images_on_page' found in chunk metadata before Pinecone upload for chunk on page {chunk.metadata.get('page')}. This should have been removed by the splitter. Removing it now.\")\n",
        "         del chunk.metadata[\"images_on_page\"]"
      ],
      "metadata": {
        "id": "81OQYS74QlOS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_chunks = [chunk for chunk in chunks if \"images\" in chunk.metadata]"
      ],
      "metadata": {
        "id": "RuxcyjJDRyMX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "for chunk in image_chunks:\n",
        "    if \"images\" in chunk.metadata and isinstance(chunk.metadata[\"images\"], dict):\n",
        "        chunk.metadata[\"images\"] = json.dumps(chunk.metadata[\"images\"])"
      ],
      "metadata": {
        "id": "PhVqP8OjRtFn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Embedding Model\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"intfloat/multilingual-e5-large-instruct\",\n",
        "    model_kwargs={'device': 'cpu'} # Use 'cuda' if GPU is available and configured\n",
        ")\n",
        "embedding_dimensions = len(embedding_model.embed_query(\"test\")) # From Phase 1"
      ],
      "metadata": {
        "id": "WMweCHU9R5xb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Pinecone Setup (assuming index 'multiagent-rag' exists and is configured)\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index_name = \"multiagent-rag\"\n",
        "pinecone_index = pc.Index(index_name)\n",
        "# If you need to re-create or ensure dimensions:\n",
        "if index_name not in [index['name'] for index in pc.list_indexes()]:\n",
        "    # Create index if it doesn't exist\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=embedding_dimensions,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(\n",
        "            cloud=\"aws\",\n",
        "            region=\"us-east-1\"\n",
        "        )\n",
        "    )\n",
        "    print(f\"Index '{index_name}' created.\")\n",
        "    pinecone_index = pc.Index(index_name)\n",
        "else:\n",
        "    pinecone_index = pc.Index(index_name)\n",
        "    print(f\"Index '{index_name}' already exists and has been initialized\")\n",
        "\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embedding_model)\n",
        "\n",
        "# IMPORTANT: If you've changed the structure of chunks or their metadata,\n",
        "# you might need to clear the old index or use a new one, and re-add documents.\n",
        "print(f\"Adding {len(chunks)} chunks to the vector store...\")\n",
        "vector_store.add_documents(chunks)\n",
        "print(\"Finished adding chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfItiPjGQuyY",
        "outputId": "5e5c8945-21b0-4214-e765-d5fa6654e1c3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 'multiagent-rag' already exists and has been initialized\n",
            "Adding 100 chunks to the vector store...\n",
            "Finished adding chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 4: Instantiate Phase 2 Agents and Orchestrator ---\n",
        "# (Assuming vector_store is initialized and populated from above or previous run)\n",
        "# Ensure RetrievalAgent is defined as in Phase 1, using the vector_store\n",
        "class RetrievalAgent:\n",
        "    def __init__(self):\n",
        "        self.embeddings = embedding_model\n",
        "        self.vector_store = vector_store # Make sure vector_store is initialized\n",
        "    def retrieve(self, query, k=3):\n",
        "        return self.vector_store.similarity_search_with_score(query, k=k)"
      ],
      "metadata": {
        "id": "12bRxu3XQ0yl"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_agent = RetrievalAgent() # Assumes vector_store is globally available or passed in\n",
        "image_agent_p2 = ImageAgent() # Phase 2 version (or refined Phase 1 version)\n",
        "image_analysis_agent_p2 = ImageAnalysisAgent()\n",
        "\n",
        "orchestrator_p2 = Phase2Orchestrator(\n",
        "    retrieval_agent,\n",
        "    image_agent_p2,\n",
        "    image_analysis_agent_p2\n",
        ")\n",
        "\n",
        "generator_p2 = EnhancedResponseGenerator(orchestrator_p2)\n",
        "\n",
        "# --- Cell 5: Test Phase 2 System ---\n",
        "question_p2 = \"Explain me Matrix-matrix product –tiling and shared memory usage\"\n",
        "print(f\"\\nProcessing P2 Question: {question_p2}\\n\")\n",
        "\n",
        "response_p2_content = generator_p2.generate_response(question_p2)\n",
        "print(\"\\n--- Phase 2 LLM Response ---\")\n",
        "print(response_p2_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3li4JASIQ4IW",
        "outputId": "12bd6b18-a2b9-4794-d79d-c2168051fc89"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing P2 Question: Explain me Matrix-matrix product –tiling and shared memory usage\n",
            "\n",
            "Warning: Missing source_file or xref for image_id img_30_1. Skipping analysis.\n",
            "Warning: Missing source_file or xref for image_id img_30_1. Skipping analysis.\n",
            "Warning: Missing source_file or xref for image_id img_29_1. Skipping analysis.\n",
            "Warning: Missing source_file or xref for image_id img_29_2. Skipping analysis.\n",
            "\n",
            "--- Phase 2 LLM Response ---\n",
            "The provided text mentions \"Matrix-matrix product – tiling and shared memory usage\" and \"Matrix-matrix product – tiling and shared memory (cont)\" from the NVIDIA-UIUC GPU teaching kit and the NVIDIA CUDA Programming Guide.  The text notes that in matrix multiplication with tiling, the tile size should match the block size and fit within shared memory.  However, no further details on the implementation or specifics of tiling and shared memory usage are available in the provided text.  The image analyses are unhelpful as they contain no relevant OCR text and are described as having unknown types and low relevance.  Therefore, a complete explanation of matrix-matrix product – tiling and shared memory usage cannot be provided based solely on the given information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Optional: For Debugging ---\n",
        "print(\"\\n--- Debugging Orchestrator P2 Output ---\")\n",
        "debug_results_p2 = orchestrator_p2.process_query(question_p2, k=2)\n",
        "print(f\"Query: {debug_results_p2['query']}\")\n",
        "print(f\"\\nRetrieved Docs ({len(debug_results_p2['retrieved_docs_with_scores'])}):\")\n",
        "for i, (doc, score) in enumerate(debug_results_p2['retrieved_docs_with_scores']):\n",
        "    print(f\"  Doc {i+1} (Page {doc.metadata.get('page', 'N/A')+1}, Score: {score:.3f}):\")\n",
        "    print(f\"    Content snippet: {doc.page_content[:200]}...\")\n",
        "    if \"images\" in doc.metadata: # This would be the JSON string from Pinecone\n",
        "        print(f\"    Raw images metadata: {doc.metadata['images'][:100]}...\")\n",
        "\n",
        "\n",
        "print(f\"\\nAnalyzed Image Info ({len(debug_results_p2['analyzed_image_info'])}):\")\n",
        "for i, img_info in enumerate(debug_results_p2['analyzed_image_info']):\n",
        "    print(f\"  Image {i+1}: {img_info.get('placeholder')}\")\n",
        "    print(f\"    Page: {img_info.get('page_num')}, XRef: {img_info.get('xref')}\")\n",
        "    print(f\"    Type: {img_info.get('image_type')}, Format: {img_info.get('image_format')}, Calculated Relevance: {img_info.get('calculated_relevance')}\")\n",
        "    print(f\"    OCR: '{str(img_info.get('ocr_text'))[:100]}...'\")\n",
        "    print(f\"    Document Context: '{img_info.get('document_context', 'N/A')[:100]}...'\")\n",
        "    if img_info.get('analysis_error'):\n",
        "        print(f\"    Analysis Error: {img_info.get('analysis_error')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78vx7q1mP0uS",
        "outputId": "cbcf8fb4-dc61-4100-8465-2532f925e865"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Debugging Orchestrator P2 Output ---\n",
            "Warning: Missing source_file or xref for image_id img_30_1. Skipping analysis.\n",
            "Warning: Missing source_file or xref for image_id img_30_1. Skipping analysis.\n",
            "Query: Explain me Matrix-matrix product –tiling and shared memory usage\n",
            "\n",
            "Retrieved Docs (2):\n",
            "  Doc 1 (Page 30.0, Score: 0.941):\n",
            "    Content snippet: Matrix-matrix product – tiling and shared memory (cont) \n",
            "Source: GPU teaching kit, NVIDIA-UIUC...\n",
            "    Raw images metadata: {\"img_30_1\": {\"image_id\": \"img_30_1\", \"page_num\": 30, \"position\": 1, \"source_file\": \"/content/GPU.pd...\n",
            "  Doc 2 (Page 30.0, Score: 0.941):\n",
            "    Content snippet: Matrix-matrix product – tiling and shared memory (cont) \n",
            "Source: GPU teaching kit, NVIDIA-UIUC...\n",
            "    Raw images metadata: {\"img_30_1\": {\"image_id\": \"img_30_1\", \"page_num\": 30, \"position\": 1, \"source_file\": \"/content/GPU.pd...\n",
            "\n",
            "Analyzed Image Info (2):\n",
            "  Image 1: [IMAGE: img_30_1]\n",
            "    Page: 30, XRef: None\n",
            "    Type: unknown, Format: None, Calculated Relevance: 0.0\n",
            "    OCR: 'None...'\n",
            "    Document Context: 'Placeholder not found in chunk context....'\n",
            "    Analysis Error: Missing source/xref\n",
            "  Image 2: [IMAGE: img_30_1]\n",
            "    Page: 30, XRef: None\n",
            "    Type: unknown, Format: None, Calculated Relevance: 0.0\n",
            "    OCR: 'None...'\n",
            "    Document Context: 'Placeholder not found in chunk context....'\n",
            "    Analysis Error: Missing source/xref\n"
          ]
        }
      ]
    }
  ]
}